path='AJA_Factiva-20180306-2258.txt'
alldata=create_text(path)
#Clean Data
given=alldata
new=gsub(pattern='\\W',replace=" ",given)
new=gsub(pattern='\\d',replace=" ",new)
new=tolower(new)
new=removeWords(new,stopwords())
new=removeWords(new,c('al','jazeera','copyright','ajazen','english','march','february'))
new=gsub(pattern='\\b[A-z]\\b{1}',replace=' ',new)
new=stripWhitespace(new)
corp=Corpus(VectorSource(new))
dtm=DocumentTermMatrix(corp)
tdm =TermDocumentMatrix(corp)
tdm_matrix=as.matrix(tdm)
dtm_matrix=as.matrix(dtm)
#calculate frequency of occurences
df=as.data.frame(tdm_matrix)
#Plot Function
colnames(df)=c('frequency')
freq=df$frequency
df$words=rownames(df)
#number of terms
total_no_terms= sum(freq)
#generating CSV file with frequencies
df_descend=df[order(df$frequency,decreasing = TRUE),]
rownames(df_descend)=1:length(df$frequency)
head(df_descend)
write.csv(df_descend, file = "frequency.csv")
#Plotting
colnames(df)=c('frequency')
df$terms=rownames(df)
plot_word(df$terms,df$frequency,70)
plot_all(df$terms,df$frequency,100)
#_____________________________________________________________________
#______________________________________________________________________
#Use the following parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 5
findFreqTerms(dtm, lowfreq = 20)
#Run LDA using Gibbs sampling
ldaoutput <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
#load all required libraries
library(tm)
library(tidytext)
library(topicmodels)
library(stringr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
sink('C:/Users/MANASA/Downloads/R Project/console-output.txt')
source('C:/Users/MANASA/Downloads/R Project/dataLoading.R')
source('C:/Users/MANASA/Downloads/R Project/plots.R')
#Function for dividing the text into articles
path='AJA_Factiva-20180306-2258.txt'
alldata=create_text(path)
#Clean Data
given=alldata
new=gsub(pattern='\\W',replace=" ",given)
new=gsub(pattern='\\d',replace=" ",new)
new=tolower(new)
new=removeWords(new,stopwords())
new=removeWords(new,c('al','jazeera','copyright','ajazen','english','march','february'))
new=gsub(pattern='\\b[A-z]\\b{1}',replace=' ',new)
new=stripWhitespace(new)
corp=Corpus(VectorSource(new))
dtm=DocumentTermMatrix(corp)
tdm =TermDocumentMatrix(corp)
tdm_matrix=as.matrix(tdm)
dtm_matrix=as.matrix(dtm)
#calculate frequency of occurences
df=as.data.frame(tdm_matrix)
#Plot Function
colnames(df)=c('frequency')
freq=df$frequency
df$words=rownames(df)
#number of terms
total_no_terms= sum(freq)
#generating CSV file with frequencies
df_descend=df[order(df$frequency,decreasing = TRUE),]
rownames(df_descend)=1:length(df$frequency)
head(df_descend)
write.csv(df_descend, file = "frequency.csv")
#Plotting
colnames(df)=c('frequency')
df$terms=rownames(df)
plot_word(df$terms,df$frequency,70)
plot_all(df$terms,df$frequency,100)
#_____________________________________________________________________
#______________________________________________________________________
#Use the following parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 5
findFreqTerms(dtm, lowfreq = 20)
#Run LDA using Gibbs sampling
ldaoutput <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
##write out results
#The LDA algorithm returns an object that contains a lot of information.
#Of particular interest to us are a) the document to topic assignments and b) the top terms in each topic
#a) docs to topics
ldaoutput.topics <- as.matrix(topics(ldaoutput))
write.csv(ldaoutput.topics,file=paste("ldamodel",k,"DocsToTopics.csv"))
#b) top 6 terms in each topic
ldaoutput.terms <- as.matrix(terms(ldaoutput,6))
write.csv(ldaoutput.terms,file=paste("ldamodel",k,"TopicsToTerms.csv"))
jpeg("mygraph.jpeg")
sink()
#load all required libraries
library(tm)
library(tidytext)
library(topicmodels)
library(stringr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
sink('C:/Users/MANASA/Downloads/R Project/console-output.txt')
source('C:/Users/MANASA/Downloads/R Project/dataLoading.R')
source('C:/Users/MANASA/Downloads/R Project/plots.R')
#Function for dividing the text into articles
path='AJA_Factiva-20180306-2258.txt'
alldata=create_text(path)
#Clean Data
given=alldata
new=gsub(pattern='\\W',replace=" ",given)
new=gsub(pattern='\\d',replace=" ",new)
new=tolower(new)
new=removeWords(new,stopwords())
new=removeWords(new,c('al','jazeera','copyright','ajazen','english','march','february'))
new=gsub(pattern='\\b[A-z]\\b{1}',replace=' ',new)
new=stripWhitespace(new)
corp=Corpus(VectorSource(new))
dtm=DocumentTermMatrix(corp)
tdm =TermDocumentMatrix(corp)
tdm_matrix=as.matrix(tdm)
dtm_matrix=as.matrix(dtm)
#calculate frequency of occurences
df=as.data.frame(tdm_matrix)
jpeg("Black.jpeg")
#Plot Function
colnames(df)=c('frequency')
freq=df$frequency
df$words=rownames(df)
#number of terms
total_no_terms= sum(freq)
#generating CSV file with frequencies
df_descend=df[order(df$frequency,decreasing = TRUE),]
rownames(df_descend)=1:length(df$frequency)
head(df_descend)
write.csv(df_descend, file = "frequency.csv")
#Plotting
colnames(df)=c('frequency')
df$terms=rownames(df)
jpeg("Color.jpeg")
plot_word(df$terms,df$frequency,70)
jpeg("Plot_all.jpeg")
plot_all(df$terms,df$frequency,100)
#_____________________________________________________________________
#______________________________________________________________________
#Use the following parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 5
findFreqTerms(dtm, lowfreq = 20)
#Run LDA using Gibbs sampling
ldaoutput <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
##write out results
#The LDA algorithm returns an object that contains a lot of information.
#Of particular interest to us are a) the document to topic assignments and b) the top terms in each topic
#a) docs to topics
ldaoutput.topics <- as.matrix(topics(ldaoutput))
write.csv(ldaoutput.topics,file=paste("ldamodel",k,"DocsToTopics.csv"))
#b) top 6 terms in each topic
ldaoutput.terms <- as.matrix(terms(ldaoutput,6))
write.csv(ldaoutput.terms,file=paste("ldamodel",k,"TopicsToTerms.csv"))
sink()
#load all required libraries
library(tm)
library(tidytext)
library(topicmodels)
library(stringr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
sink('C:/Users/MANASA/Downloads/R Project/console-output.txt')
source('C:/Users/MANASA/Downloads/R Project/dataLoading.R')
source('C:/Users/MANASA/Downloads/R Project/plots.R')
#Function for dividing the text into articles
path='AJA_Factiva-20180306-2258.txt'
alldata=create_text(path)
#Clean Data
given=alldata
new=gsub(pattern='\\W',replace=" ",given)
new=gsub(pattern='\\d',replace=" ",new)
new=tolower(new)
new=removeWords(new,stopwords())
new=removeWords(new,c('al','jazeera','copyright','ajazen','english','march','february'))
new=gsub(pattern='\\b[A-z]\\b{1}',replace=' ',new)
new=stripWhitespace(new)
corp=Corpus(VectorSource(new))
dtm=DocumentTermMatrix(corp)
tdm =TermDocumentMatrix(corp)
tdm_matrix=as.matrix(tdm)
dtm_matrix=as.matrix(dtm)
#calculate frequency of occurences
df=as.data.frame(tdm_matrix)
png("Black.png")
#Plot Function
colnames(df)=c('frequency')
freq=df$frequency
df$words=rownames(df)
#number of terms
total_no_terms= sum(freq)
#generating CSV file with frequencies
df_descend=df[order(df$frequency,decreasing = TRUE),]
rownames(df_descend)=1:length(df$frequency)
head(df_descend)
write.csv(df_descend, file = "frequency.csv")
#Plotting
colnames(df)=c('frequency')
df$terms=rownames(df)
png("Color.png")
plot_word(df$terms,df$frequency,70)
png("Plot_all.png")
plot_all(df$terms,df$frequency,100)
#_____________________________________________________________________
#______________________________________________________________________
#Use the following parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 5
findFreqTerms(dtm, lowfreq = 20)
#Run LDA using Gibbs sampling
ldaoutput <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
##write out results
#The LDA algorithm returns an object that contains a lot of information.
#Of particular interest to us are a) the document to topic assignments and b) the top terms in each topic
#a) docs to topics
ldaoutput.topics <- as.matrix(topics(ldaoutput))
write.csv(ldaoutput.topics,file=paste("ldamodel",k,"DocsToTopics.csv"))
#b) top 6 terms in each topic
ldaoutput.terms <- as.matrix(terms(ldaoutput,6))
write.csv(ldaoutput.terms,file=paste("ldamodel",k,"TopicsToTerms.csv"))
sink()
#load all required libraries
library(tm)
library(tidytext)
library(topicmodels)
library(stringr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
sink('C:/Users/MANASA/Downloads/R Project/console-output.txt')
source('C:/Users/MANASA/Downloads/R Project/dataLoading.R')
source('C:/Users/MANASA/Downloads/R Project/plots.R')
#Function for dividing the text into articles
path='AJA_Factiva-20180306-2258.txt'
alldata=create_text(path)
#Clean Data
given=alldata
new=gsub(pattern='\\W',replace=" ",given)
new=gsub(pattern='\\d',replace=" ",new)
new=tolower(new)
new=removeWords(new,stopwords())
new=removeWords(new,c('al','jazeera','copyright','ajazen','english','march','february'))
new=gsub(pattern='\\b[A-z]\\b{1}',replace=' ',new)
new=stripWhitespace(new)
corp=Corpus(VectorSource(new))
dtm=DocumentTermMatrix(corp)
tdm =TermDocumentMatrix(corp)
tdm_matrix=as.matrix(tdm)
dtm_matrix=as.matrix(dtm)
#calculate frequency of occurences
df=as.data.frame(tdm_matrix)
png("Black.png")
#Plot Function
colnames(df)=c('frequency')
freq=df$frequency
df$words=rownames(df)
dev.off()
#number of terms
total_no_terms= sum(freq)
#generating CSV file with frequencies
df_descend=df[order(df$frequency,decreasing = TRUE),]
rownames(df_descend)=1:length(df$frequency)
head(df_descend)
write.csv(df_descend, file = "frequency.csv")
#Plotting
colnames(df)=c('frequency')
df$terms=rownames(df)
png("Color.png")
plot_word(df$terms,df$frequency,70)
dev.off()
png("Plot_all.png")
plot_all(df$terms,df$frequency,100)
dev.off()
#_____________________________________________________________________
#______________________________________________________________________
#Use the following parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 5
findFreqTerms(dtm, lowfreq = 20)
#Run LDA using Gibbs sampling
ldaoutput <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
##write out results
#The LDA algorithm returns an object that contains a lot of information.
#Of particular interest to us are a) the document to topic assignments and b) the top terms in each topic
#a) docs to topics
ldaoutput.topics <- as.matrix(topics(ldaoutput))
write.csv(ldaoutput.topics,file=paste("ldamodel",k,"DocsToTopics.csv"))
#b) top 6 terms in each topic
ldaoutput.terms <- as.matrix(terms(ldaoutput,6))
write.csv(ldaoutput.terms,file=paste("ldamodel",k,"TopicsToTerms.csv"))
sink()
#load all required libraries
library(tm)
library(tidytext)
library(topicmodels)
library(stringr)
library(wordcloud)
library(SnowballC)
library(RColorBrewer)
library(ggplot2)
sink('C:/Users/MANASA/Downloads/R Project/console-output.txt')
source('C:/Users/MANASA/Downloads/R Project/dataLoading.R')
source('C:/Users/MANASA/Downloads/R Project/plots.R')
#Function for dividing the text into articles
path='AJA_Factiva-20180306-2258.txt'
alldata=create_text(path)
#Clean Data
given=alldata
new=gsub(pattern='\\W',replace=" ",given)
new=gsub(pattern='\\d',replace=" ",new)
new=tolower(new)
new=removeWords(new,stopwords())
new=removeWords(new,c('al','jazeera','copyright','ajazen','english','march','february'))
new=gsub(pattern='\\b[A-z]\\b{1}',replace=' ',new)
new=stripWhitespace(new)
corp=Corpus(VectorSource(new))
dtm=DocumentTermMatrix(corp)
tdm =TermDocumentMatrix(corp)
tdm_matrix=as.matrix(tdm)
dtm_matrix=as.matrix(dtm)
#calculate frequency of occurences
df=as.data.frame(tdm_matrix)
#Plot Function
colnames(df)=c('frequency')
freq=df$frequency
df$words=rownames(df)
dev.off()
#number of terms
total_no_terms= sum(freq)
#generating CSV file with frequencies
df_descend=df[order(df$frequency,decreasing = TRUE),]
rownames(df_descend)=1:length(df$frequency)
head(df_descend)
write.csv(df_descend, file = "frequency.csv")
#Plotting
colnames(df)=c('frequency')
df$terms=rownames(df)
plot_word(df$terms,df$frequency,70)
plot_all(df$terms,df$frequency,100)
#_____________________________________________________________________
#______________________________________________________________________
#Use the following parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 5
findFreqTerms(dtm, lowfreq = 20)
#Run LDA using Gibbs sampling
ldaoutput <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
sink('C:/Users/MANASA/Downloads/R Project/console-output.txt')
source('C:/Users/MANASA/Downloads/R Project/dataLoading.R')
source('C:/Users/MANASA/Downloads/R Project/plots.R')
#Function for dividing the text into articles
path='AJA_Factiva-20180306-2258.txt'
alldata=create_text(path)
#Clean Data
given=alldata
new=gsub(pattern='\\W',replace=" ",given)
new=gsub(pattern='\\d',replace=" ",new)
new=tolower(new)
new=removeWords(new,stopwords())
new=removeWords(new,c('al','jazeera','copyright','ajazen','english','march','february'))
new=gsub(pattern='\\b[A-z]\\b{1}',replace=' ',new)
new=stripWhitespace(new)
corp=Corpus(VectorSource(new))
dtm=DocumentTermMatrix(corp)
tdm =TermDocumentMatrix(corp)
tdm_matrix=as.matrix(tdm)
dtm_matrix=as.matrix(dtm)
#calculate frequency of occurences
df=as.data.frame(tdm_matrix)
#Plot Function
colnames(df)=c('frequency')
freq=df$frequency
df$words=rownames(df)
#number of terms
total_no_terms= sum(freq)
#generating CSV file with frequencies
df_descend=df[order(df$frequency,decreasing = TRUE),]
rownames(df_descend)=1:length(df$frequency)
head(df_descend)
write.csv(df_descend, file = "frequency.csv")
#Plotting
colnames(df)=c('frequency')
df$terms=rownames(df)
plot_word(df$terms,df$frequency,70)
plot_all(df$terms,df$frequency,100)
#Use the following parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 5
findFreqTerms(dtm, lowfreq = 20)
#Run LDA using Gibbs sampling
ldaoutput <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
#a) docs to topics
ldaoutput.topics <- as.matrix(topics(ldaoutput))
write.csv(ldaoutput.topics,file=paste("ldamodel",k,"DocsToTopics.csv"))
View(ldaoutput)
p2
p2=wordcloud(terms,freq,min.freq = min_freq_wordcloud,random.order=FALSE,scale=c(3,0.5))
source('C:/Users/MANASA/Downloads/R Project/dataLoading.R')
source('C:/Users/MANASA/Downloads/R Project/plots.R')
#Function for dividing the text into articles
path='AJA_Factiva-20180306-2258.txt'
alldata=create_text(path)
#Clean Data
given=alldata
new=gsub(pattern='\\W',replace=" ",given)
new=gsub(pattern='\\d',replace=" ",new)
new=tolower(new)
new=removeWords(new,stopwords())
new=removeWords(new,c('al','jazeera','copyright','ajazen','english','march','february'))
new=gsub(pattern='\\b[A-z]\\b{1}',replace=' ',new)
new=stripWhitespace(new)
corp=Corpus(VectorSource(new))
dtm=DocumentTermMatrix(corp)
tdm =TermDocumentMatrix(corp)
tdm_matrix=as.matrix(tdm)
dtm_matrix=as.matrix(dtm)
#calculate frequency of occurences
df=as.data.frame(tdm_matrix)
#Plot Function
colnames(df)=c('frequency')
freq=df$frequency
df$words=rownames(df)
#number of terms
total_no_terms= sum(freq)
#generating CSV file with frequencies
df_descend=df[order(df$frequency,decreasing = TRUE),]
rownames(df_descend)=1:length(df$frequency)
head(df_descend)
write.csv(df_descend, file = "frequency.csv")
#Plotting
colnames(df)=c('frequency')
df$terms=rownames(df)
plot_word(df$terms,df$frequency,70)
plot_all(df$terms,df$frequency,100)
#Use the following parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
#Number of topics
k <- 5
findFreqTerms(dtm, lowfreq = 20)
#Run LDA using Gibbs sampling
ldaoutput <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
